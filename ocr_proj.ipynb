{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0ff9a2",
   "metadata": {},
   "source": [
    "# OCR for IAM lines dataset pipeline \n",
    "\n",
    "This notebook presents a complete end-to-end pipeline for Optical Character Recognition (OCR) on the IAM Lines dataset, a widely used benchmark for handwritten text recognition. The pipeline covers all key stages of building an OCR system: data exploration and preprocessing, augmentation, dataset preparation, model design (a CRNN with attention), training with CTC loss, and evaluation using beam search with a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe932bb1",
   "metadata": {},
   "source": [
    "# Installing libraries\n",
    "These commands install the core dependencies required for the OCR pipeline. The datasets library provides easy access to the IAM Lines dataset, while editdistance is used for computing accuracy metrics, albumentations supports image augmentation, pyctcdecode enables CTC decoding with beam search and kenlm is used to integrate a language model for improved recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438c318",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install datasets editdistance albumentations\n",
    "!pip install pyctcdecode\n",
    "!pip install kenlm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68054d10",
   "metadata": {},
   "source": [
    "# Parameter and vocabulary configuration\n",
    "\n",
    "This configuration block defines the core settings for training and preprocessing. It sets hyperparameters such as learning rate, max number of epochs, batch size, weight decay, and dropout, which control how the model learns. It also specifies image dimensions for consistent preprocessing and builds the vocabulary with character-to-index mappings, which are essential for encoding handwritten text into model-readable form and decoding predictions back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d4633",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 80\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Image preprocessing\n",
    "TARGET_HEIGHT = 128\n",
    "TARGET_WIDTH = 1028\n",
    "\n",
    "# Vocabulary\n",
    "VOCAB = list(\"!#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz \")\n",
    "VOCAB = ['<BLANK>'] + VOCAB\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "# Create mappings\n",
    "CHAR_TO_IDX = {char: idx for idx, char in enumerate(VOCAB)}\n",
    "IDX_TO_CHAR = {idx: char for char, idx in CHAR_TO_IDX.items()}\n",
    "CHARS = VOCAB[1:]  # Remove <BLANK> for pyctcdecode\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import tempfile\n",
    "import logging\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import editdistance\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up environment variables and CUDA settings.\"\"\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfccc52",
   "metadata": {},
   "source": [
    "# Data Analysis \n",
    "\n",
    "The functions show_random_samples and analyze_dataset provide essential exploratory data analysis for the IAM dataset. By visualizing random handwritten text samples, we can qualitatively assess the dataset’s quality and variety. The analysis further explores character frequency, line length distribution, image dimensions, unique character sets, and common words, offering valuable insights into the dataset’s structure. This understanding guides vocabulary design, preprocessing strategies, and model configuration choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74626954",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def show_random_samples(dataset, n=5):\n",
    "    \"\"\"Display random image samples from dataset.\"\"\"\n",
    "    samples = dataset.shuffle(seed=42).select(range(n))\n",
    "    for i, sample in enumerate(samples):\n",
    "        plt.figure(figsize=(8, 2))\n",
    "        plt.imshow(sample[\"image\"], cmap=\"gray\")\n",
    "        plt.title(f\"Text: {sample['text']}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "def analyze_dataset(dataset):\n",
    "    \"\"\"Perform exploratory data analysis on the dataset.\"\"\"\n",
    "    # Character frequency analysis\n",
    "    char_counts = Counter()\n",
    "    for sample in dataset:  \n",
    "        char_counts.update(sample[\"text\"])\n",
    "\n",
    "    chars, freqs = zip(*char_counts.most_common())\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(chars, freqs)\n",
    "    plt.title(\"Most Common Characters in Dataset\")\n",
    "    plt.xlabel(\"Character\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Line length distribution\n",
    "    line_lengths = [len(sample[\"text\"]) for sample in dataset]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(line_lengths, bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.title(\"Distribution of Line Text Lengths\")\n",
    "    plt.xlabel(\"Number of Characters\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(f\"Average line length: {np.mean(line_lengths):.2f}\")\n",
    "\n",
    "    # Image size analysis (sample)\n",
    "    widths, heights = [], []\n",
    "    for sample in dataset.select(range(500)):  \n",
    "        w, h = sample[\"image\"].size\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(widths, bins=30, alpha=0.7, label=\"Width\")\n",
    "    plt.hist(heights, bins=30, alpha=0.7, label=\"Height\")\n",
    "    plt.title(\"Image Width and Height Distribution (First 500 Samples)\")\n",
    "    plt.xlabel(\"Pixels\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(f\"Average Width: {np.mean(widths):.1f}, Average Height: {np.mean(heights):.1f}\")\n",
    "\n",
    "    # Unique characters\n",
    "    unique_chars = sorted(set(''.join([s[\"text\"] for s in dataset])))  \n",
    "    print(\"Unique Characters Found:\")\n",
    "    print(\"\".join(unique_chars))\n",
    "    print(f\"Total Unique Characters: {len(unique_chars)}\")\n",
    "\n",
    "    # Word frequency\n",
    "    word_counts = Counter(chain.from_iterable(s[\"text\"].split() for s in dataset))  \n",
    "    print(\"Top 10 most common words:\")\n",
    "    print(word_counts.most_common(10))\n",
    "    print(f\"Total unique words: {len(word_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13067149",
   "metadata": {},
   "source": [
    "# Image augmentations\n",
    "\n",
    "This block defines the data augmentation pipeline using Albumentations, a library for efficient image transformations. The pipeline introduces controlled variations such as slight rotations, brightness/contrast adjustments, elastic deformations, grid distortions, noise, and motion blur. These augmentations simulate the natural variability in handwriting and scanning conditions, making the model more robust and better at generalizing to unseen handwriting styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a630a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "\n",
    "# Augmentation pipeline\n",
    "ALBUMENTATIONS_TRANSFORM = A.Compose([\n",
    "    A.Rotate(limit=2, border_mode=cv2.BORDER_CONSTANT, p=0.4),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, p=0.3),\n",
    "    A.GridDistortion(num_steps=5, distort_limit=0.1, p=0.3),\n",
    "    A.GaussNoise(noise_scale_factor=0.1, p=0.2),\n",
    "    A.MotionBlur(blur_limit=3, p=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d2167",
   "metadata": {},
   "source": [
    "# Image preprocessing and showing of preprocessed image samples\n",
    "\n",
    "This section defines the preprocessing pipeline that prepares raw handwritten line images for training. The preprocess_image function standardizes inputs by converting them to grayscale, handling invalid cases, applying optional contrast enhancement, and supporting several binarization methods (e.g., Otsu, adaptive mean, adaptive Gaussian). It then resizes and pads images to fixed dimensions while preserving aspect ratio, and normalizes pixel values to either [0,1] or [-1,1]. The show_preprocessed_samples helper function visualizes original images alongside their preprocessed versions, making it easier to verify the effects of preprocessing choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f34c65",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img, target_height=128, target_width=1028,\n",
    "                     enhance_contrast=False, normalization_range=\"0_1\",\n",
    "                     log_rescaling=True, binarization=None, binary_threshold=127,\n",
    "                     adaptive_threshold_block_size=11, adaptive_threshold_c=2):\n",
    "    \"\"\"\n",
    "    Preprocess image for IAM OCR.\n",
    "    \n",
    "    Args:\n",
    "        img: PIL Image or numpy array\n",
    "        target_height: Target height in pixels\n",
    "        target_width: Target width in pixels\n",
    "        enhance_contrast: Whether to apply contrast enhancement\n",
    "        normalization_range: \"0_1\" for [0,1] or \"-1_1\" for [-1,1]\n",
    "        log_rescaling: Whether to log rescaling operations\n",
    "        binarization: Binarization method (None, \"otsu\", \"adaptive_mean\", etc.)\n",
    "        binary_threshold: Threshold for simple binarization\n",
    "        adaptive_threshold_block_size: Block size for adaptive thresholding\n",
    "        adaptive_threshold_c: Constant for adaptive thresholding\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image as numpy array with shape (height, width, 1)\n",
    "    \"\"\"\n",
    "    # Convert to grayscale numpy array\n",
    "    if isinstance(img, Image.Image):\n",
    "        img = np.array(img.convert(\"L\"))\n",
    "    elif len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if img.size == 0:\n",
    "        logging.warning(\"Empty image provided, returning zero array\")\n",
    "        return np.zeros((target_height, target_width, 1), dtype=np.float32)\n",
    "    \n",
    "    h, w = img.shape\n",
    "    if h == 0 or w == 0:\n",
    "        logging.warning(f\"Invalid image dimensions: {h}x{w}, returning zero array\")\n",
    "        return np.zeros((target_height, target_width, 1), dtype=np.float32)\n",
    "    \n",
    "    # Optional contrast enhancement\n",
    "    if enhance_contrast:\n",
    "        img = cv2.convertScaleAbs(img, alpha=1.1, beta=5)\n",
    "    \n",
    "    # Apply binarization\n",
    "    if binarization == \"otsu\":\n",
    "        _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    elif binarization == \"adaptive_mean\":\n",
    "        img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "                                   cv2.THRESH_BINARY, adaptive_threshold_block_size, \n",
    "                                   adaptive_threshold_c)\n",
    "    elif binarization == \"adaptive_gaussian\":\n",
    "        img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                   cv2.THRESH_BINARY, adaptive_threshold_block_size, \n",
    "                                   adaptive_threshold_c)\n",
    "    elif binarization == \"simple\":\n",
    "        _, img = cv2.threshold(img, binary_threshold, 255, cv2.THRESH_BINARY)\n",
    "    elif binarization is not None:\n",
    "        raise ValueError(\"Invalid binarization method\")\n",
    "    \n",
    "    # Calculate scaling factors\n",
    "    scale_h = target_height / h\n",
    "    scale_w = target_width / w\n",
    "    scale = min(scale_h, scale_w)\n",
    "    \n",
    "    new_h = int(h * scale)\n",
    "    new_w = int(w * scale)\n",
    "    \n",
    "    # Log aspect ratio changes\n",
    "    if log_rescaling:\n",
    "        original_aspect = w / h\n",
    "        target_aspect = target_width / target_height\n",
    "        if abs(original_aspect - target_aspect) / target_aspect > 0.1:\n",
    "            logging.info(f\"Image rescaled from {w}x{h} to {new_w}x{new_h}\")\n",
    "    \n",
    "    # Resize and pad\n",
    "    img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    padded = np.full((target_height, target_width), 255, dtype=np.uint8)\n",
    "    \n",
    "    y_offset = (target_height - new_h) // 2\n",
    "    x_offset = (target_width - new_w) // 2\n",
    "    padded[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = img\n",
    "    \n",
    "    # Normalize\n",
    "    img = padded.astype(np.float32) / 255.0\n",
    "    if normalization_range == \"-1_1\":\n",
    "        img = (img - 0.5) * 2\n",
    "    elif normalization_range != \"0_1\":\n",
    "        raise ValueError(\"normalization_range must be '0_1' or '-1_1'\")\n",
    "    \n",
    "    return np.expand_dims(img, axis=-1)\n",
    "\n",
    "def show_preprocessed_samples(dataset, n=5):\n",
    "    \"\"\"Show original vs preprocessed images.\"\"\"\n",
    "    samples = dataset.shuffle(seed=42).select(range(n))\n",
    "    \n",
    "    for sample in samples:\n",
    "        original_img = sample[\"image\"]\n",
    "        text = sample[\"text\"]\n",
    "        \n",
    "        processed_img = preprocess_image(\n",
    "            original_img, 128, 1028, True, \"0_1\", True, \"adaptive_gaussian\"\n",
    "        )\n",
    "        processed_img_vis = processed_img.squeeze()\n",
    "        \n",
    "        plt.figure(figsize=(12, 3))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_img, cmap=\"gray\")\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(processed_img_vis, cmap=\"gray\")\n",
    "        plt.title(f\"Preprocessed\\n{text}\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383d51c",
   "metadata": {},
   "source": [
    "# Dataset wrapper class and functions\n",
    "\n",
    "This part defines how the IAM dataset is wrapped and prepared for training. The IAMDataset class extends PyTorch’s Dataset and handles preprocessing, augmentation, and encoding text labels into numerical indices. It also filters out samples with unsupported characters or overly long text. Each item returns the preprocessed image tensor along with its text and target sequence. The collate_fn function ensures variable-length text sequences are batched properly by concatenating targets and storing their lengths. Finally, create_dataloaders constructs efficient PyTorch dataloaders for both training and validation, handling batching, shuffling, and parallel data loading. This structure ensures seamless integration between the dataset and the model training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14010fe9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for IAM handwritten text recognition.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hf_dataset,\n",
    "                 char_to_idx: Dict[str, int],\n",
    "                 idx_to_char: Dict[int, str],\n",
    "                 target_height: int = 128,\n",
    "                 target_width: int = 1028,\n",
    "                 enhance_contrast: bool = False,\n",
    "                 normalization_range: str = \"0_1\",\n",
    "                 max_text_length: Optional[int] = None,\n",
    "                 augment: bool = False,\n",
    "                 log_rescaling: bool = True, \n",
    "                 binarization=None, \n",
    "                 binary_threshold=127,\n",
    "                 adaptive_threshold_block_size=11, \n",
    "                 adaptive_threshold_c=2):\n",
    "        \n",
    "        self.dataset = hf_dataset\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "        self.enhance_contrast = enhance_contrast\n",
    "        self.normalization_range = normalization_range\n",
    "        self.max_text_length = max_text_length\n",
    "        self.augment = augment\n",
    "        self.log_rescaling = log_rescaling\n",
    "        self.binarization = binarization\n",
    "        self.binary_threshold = binary_threshold\n",
    "        self.adaptive_threshold_block_size = adaptive_threshold_block_size\n",
    "        self.adaptive_threshold_c = adaptive_threshold_c\n",
    "            \n",
    "        # Filter by max text length\n",
    "        if max_text_length:\n",
    "            self.dataset = self.dataset.filter(\n",
    "                lambda x: len(x[\"text\"]) <= max_text_length\n",
    "            )\n",
    "            print(f\"Filtered dataset to {len(self.dataset)} samples\")\n",
    "        \n",
    "        # Filter samples that can't be encoded\n",
    "        self.valid_indices = []\n",
    "        for idx in range(len(self.dataset)):\n",
    "            text = self.dataset[idx][\"text\"]\n",
    "            if self._can_encode_text(text):\n",
    "                self.valid_indices.append(idx)\n",
    "        \n",
    "        print(f\"Valid samples: {len(self.valid_indices)}/{len(self.dataset)}\")\n",
    "    \n",
    "    def _can_encode_text(self, text: str) -> bool:\n",
    "        \"\"\"Check if text can be encoded with current vocabulary.\"\"\"\n",
    "        return all(char in self.char_to_idx for char in text)\n",
    "    \n",
    "    def _encode_text(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text to indices.\"\"\"\n",
    "        return [self.char_to_idx[char] for char in text if char in self.char_to_idx]\n",
    "    \n",
    "    def _preprocess_image(self, img):\n",
    "        \"\"\"Preprocess image with optional augmentation.\"\"\"\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img.convert(\"L\"))\n",
    "        elif len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        if img.dtype != np.uint8:\n",
    "            img = img.astype(np.uint8)\n",
    "\n",
    "        if self.augment:\n",
    "            img = ALBUMENTATIONS_TRANSFORM(image=img)[\"image\"]\n",
    "\n",
    "        return preprocess_image(\n",
    "            img,\n",
    "            target_height=self.target_height,\n",
    "            target_width=self.target_width,\n",
    "            enhance_contrast=self.enhance_contrast,\n",
    "            normalization_range=self.normalization_range,\n",
    "            log_rescaling=self.log_rescaling,\n",
    "            binarization=self.binarization,\n",
    "            binary_threshold=self.binary_threshold,\n",
    "            adaptive_threshold_block_size=self.adaptive_threshold_block_size,\n",
    "            adaptive_threshold_c=self.adaptive_threshold_c\n",
    "        )\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        real_idx = self.valid_indices[idx]\n",
    "        sample = self.dataset[real_idx]\n",
    "        \n",
    "        image = self._preprocess_image(sample[\"image\"])\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        \n",
    "        text = sample[\"text\"]\n",
    "        target = self._encode_text(text)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'target': torch.tensor(target, dtype=torch.long),\n",
    "            'target_length': torch.tensor(len(target), dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Collate function for DataLoader.\"\"\"\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    texts = [item['text'] for item in batch]\n",
    "    targets = [item['target'] for item in batch]\n",
    "    target_lengths = torch.stack([item['target_length'] for item in batch])\n",
    "    targets_concat = torch.cat(targets)\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'texts': texts,\n",
    "        'targets': targets_concat,\n",
    "        'target_lengths': target_lengths\n",
    "    }\n",
    "\n",
    "def create_dataloaders(train_dataset, val_dataset, batch_size=8, num_workers=4):\n",
    "    \"\"\"Create train and validation dataloaders.\"\"\"\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c8662",
   "metadata": {},
   "source": [
    "# CRNN architecture with Spatial Attention\n",
    "\n",
    "The CRNN model combines convolutional layers, optional spatial attention, and recurrent layers to recognize handwritten text. The convolutional backbone with residual blocks extracts hierarchical visual features and compresses the image into a sequence representation. A multi-head attention module can be applied to emphasize informative regions. The extracted features are then processed by stacked bidirectional LSTMs, which capture sequential dependencies across the text line. Finally, a linear classifier maps the sequence outputs to the vocabulary, producing log-probabilities suitable for CTC loss training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11d113",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import math\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with GELU activation.\"\"\"\n",
    "    \n",
    "    def __init__(self, main_path, shortcut):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main_path = main_path\n",
    "        self.shortcut = shortcut\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.gelu(self.main_path(x) + self.shortcut(x))\n",
    "\n",
    "class MultiHeadSpatialAttention(nn.Module):\n",
    "    \"\"\"Multi-head spatial attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, channels, num_heads=4):\n",
    "        super(MultiHeadSpatialAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.channels = channels\n",
    "        self.head_dim = channels // num_heads\n",
    "        \n",
    "        assert channels % num_heads == 0, \"channels must be divisible by num_heads\"\n",
    "        \n",
    "        self.query_conv = nn.Conv2d(channels, channels, 1)\n",
    "        self.key_conv = nn.Conv2d(channels, channels, 1)\n",
    "        self.value_conv = nn.Conv2d(channels, channels, 1)\n",
    "        self.output_conv = nn.Conv2d(channels, channels, 1)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        q = self.query_conv(x).view(batch_size, self.num_heads, self.head_dim, height * width)\n",
    "        k = self.key_conv(x).view(batch_size, self.num_heads, self.head_dim, height * width)\n",
    "        v = self.value_conv(x).view(batch_size, self.num_heads, self.head_dim, height * width)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        q = q.permute(0, 1, 3, 2)  \n",
    "        k = k.permute(0, 1, 2, 3)  \n",
    "        v = v.permute(0, 1, 3, 2)  \n",
    "        \n",
    "        # Compute attention\n",
    "        attention_weights = torch.matmul(q, k) / math.sqrt(self.head_dim)\n",
    "        attention_weights = self.softmax(attention_weights)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        attended_values = torch.matmul(attention_weights, v)\n",
    "        attended_values = attended_values.permute(0, 1, 3, 2).contiguous()\n",
    "        attended_values = attended_values.view(batch_size, channels, height, width)\n",
    "        \n",
    "        output = self.output_conv(attended_values)\n",
    "        return output + x  # Residual connection\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"CRNN model for handwriting recognition.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=80, hidden_size=256, num_lstm_layers=2, \n",
    "                 dropout=0.2, use_attention=True, attention_heads=4):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.use_attention = use_attention\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # CNN backbone\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # Block 1: 128x1024 -> 64x512\n",
    "            self._make_residual_block(64, 64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 2: 64x512 -> 32x256  \n",
    "            self._make_residual_block(64, 128),\n",
    "            self._make_residual_block(128, 128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 3: 32x256 -> 16x256\n",
    "            self._make_residual_block(128, 256),\n",
    "            self._make_residual_block(256, 256),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            \n",
    "            # Block 4: 16x256 -> 8x256\n",
    "            self._make_residual_block(256, 512),\n",
    "            self._make_residual_block(512, 512),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            \n",
    "            # Final compression: 8x256 -> 1x256\n",
    "            nn.Conv2d(512, 512, (3, 1), (2, 1), (1, 0)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(512, 512, (3, 1), (2, 1), (1, 0)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.GELU(),\n",
    "            nn.AdaptiveAvgPool2d((1, None)),\n",
    "        )\n",
    "        \n",
    "        # Spatial attention\n",
    "        if self.use_attention:\n",
    "            self.multihead_attention = MultiHeadSpatialAttention(512, attention_heads)\n",
    "        \n",
    "        # RNN layers\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        self.dropout_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_lstm_layers):\n",
    "            input_size = 512 if i == 0 else hidden_size\n",
    "            \n",
    "            self.rnn_layers.append(\n",
    "                nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "            )\n",
    "            \n",
    "            self.dropout_layers.append(nn.ModuleDict({\n",
    "                'linear': nn.Linear(hidden_size * 2, hidden_size),\n",
    "                'dropout': nn.Dropout(dropout)\n",
    "            }))\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_size, vocab_size)\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_residual_block(self, in_channels, out_channels, stride=1):\n",
    "        \"\"\"Create residual block.\"\"\"\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        ]\n",
    "        main_path = nn.Sequential(*layers)\n",
    "        \n",
    "        shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        return ResidualBlock(main_path, shortcut)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='linear')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='linear')\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LSTM):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        init.orthogonal_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        init.constant_(param, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # CNN feature extraction\n",
    "        conv_features = self.conv_layers(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        if self.use_attention:\n",
    "            conv_features = self.multihead_attention(conv_features)\n",
    "        \n",
    "        batch_size, channels, height, width = conv_features.size()\n",
    "        assert height == 1, f\"Height should be 1 after CNN, got {height}\"\n",
    "        \n",
    "        rnn_input = conv_features.squeeze(2).permute(0, 2, 1)\n",
    "        \n",
    "        # RNN layers\n",
    "        rnn_output = rnn_input\n",
    "        for i in range(self.num_lstm_layers):\n",
    "            lstm_out, _ = self.rnn_layers[i](rnn_output)\n",
    "            rnn_output = self.dropout_layers[i]['linear'](lstm_out)\n",
    "            rnn_output = self.dropout_layers[i]['dropout'](rnn_output)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(rnn_output)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e84f7",
   "metadata": {},
   "source": [
    "# Setting up KenLM language model\n",
    "\n",
    "This section sets up beam search decoding with a language model to improve transcription quality. First, we build a KenLM n-gram model from a cleaned text corpus that combines IAM training texts and optionally WikiText. The setup_kenlm and create_kenlm_from_corpus functions handle installing KenLM, preparing the corpus, and training an n-gram model. The setup_beam_search_decoder function integrates this language model into a CTC beam search decoder using pyctcdecode, enabling the model to generate more linguistically plausible outputs compared to greedy decoding. Finally, the wbs_decode_batch function applies this decoder to batched log-probabilities, producing refined text predictions with temperature scaling and proper handling of blank tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b5afe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "import kenlm\n",
    "\n",
    "def setup_kenlm():\n",
    "    \"\"\"Setup KenLM for language modeling.\"\"\"\n",
    "    print(\"Installing dependencies...\")\n",
    "    os.system(\"apt-get update -qq\")\n",
    "    os.system(\"apt-get install -y build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\")\n",
    "    \n",
    "    print(\"Cloning and building KenLM...\")\n",
    "    if not os.path.exists(\"/kaggle/working/kenlm\"):\n",
    "        os.system(\"git clone https://github.com/kpu/kenlm.git\")\n",
    "    os.system(\"cd kenlm && mkdir -p build && cd build && cmake .. && make -j$(nproc)\")\n",
    "    \n",
    "    kenlm_path = \"/kaggle/working/kenlm/build/bin\"\n",
    "    lmplz_path = os.path.join(kenlm_path, \"lmplz\")\n",
    "    \n",
    "    if os.path.exists(lmplz_path):\n",
    "        print(f\"lmplz found at: {lmplz_path}\")\n",
    "        return lmplz_path\n",
    "    else:\n",
    "        print(\"lmplz not found.\")\n",
    "        return None\n",
    "\n",
    "def clean_text_for_kenlm(text):\n",
    "    \"\"\"Clean text for KenLM compatibility.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove problematic tokens\n",
    "    text = text.replace('<unk>', ' ')\n",
    "    text = text.replace('<UNK>', ' ')\n",
    "    text = text.replace('<s>', ' ')\n",
    "    text = text.replace('</s>', ' ')\n",
    "    text = text.replace('<pad>', ' ')\n",
    "    text = text.replace('<PAD>', ' ')\n",
    "    \n",
    "    # Clean whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Keep only vocabulary characters\n",
    "    allowed_chars = set(\"!#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz \")\n",
    "    text = ''.join(c for c in text if c in allowed_chars)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_kenlm_from_corpus(corpus_text, order=3, lmplz_path=None):\n",
    "    \"\"\"Create KenLM model from text corpus.\"\"\"\n",
    "    corpus_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False)\n",
    "    model_file = tempfile.NamedTemporaryFile(mode='w', suffix='.arpa', delete=False)\n",
    "    \n",
    "    try:\n",
    "        # Write corpus\n",
    "        print(\"Writing corpus to temporary file...\")\n",
    "        corpus_file.write(corpus_text)\n",
    "        corpus_file.close()\n",
    "        \n",
    "        print(f\"Corpus file size: {os.path.getsize(corpus_file.name)} bytes\")\n",
    "        \n",
    "        # Find lmplz executable\n",
    "        if lmplz_path and os.path.exists(lmplz_path):\n",
    "            lmplz_cmd = lmplz_path\n",
    "        else:\n",
    "            possible_paths = [\n",
    "                \"/kaggle/working/kenlm/build/bin/lmplz\",\n",
    "                \"/kaggle/working/kenlm/bin/lmplz\",\n",
    "                \"lmplz\"\n",
    "            ]\n",
    "            \n",
    "            lmplz_cmd = None\n",
    "            for path in possible_paths:\n",
    "                if os.path.exists(path) or path == \"lmplz\":\n",
    "                    lmplz_cmd = path\n",
    "                    break\n",
    "            \n",
    "            if lmplz_cmd is None:\n",
    "                raise FileNotFoundError(\"lmplz executable not found!\")\n",
    "        \n",
    "        # Build language model\n",
    "        cmd = f\"{lmplz_cmd} -o {order} --discount_fallback --skip_symbols < {corpus_file.name} > {model_file.name}\"\n",
    "        print(f\"Running: {cmd}\")\n",
    "        \n",
    "        result = os.system(cmd)\n",
    "        \n",
    "        if result != 0:\n",
    "            raise RuntimeError(f\"lmplz failed with return code {result}\")\n",
    "        \n",
    "        if not os.path.exists(model_file.name) or os.path.getsize(model_file.name) == 0:\n",
    "            raise RuntimeError(\"lmplz produced empty model file\")\n",
    "        \n",
    "        print(f\"Model created successfully: {model_file.name}\")\n",
    "        print(f\"Model file size: {os.path.getsize(model_file.name)} bytes\")\n",
    "        \n",
    "        return model_file.name\n",
    "        \n",
    "    except Exception as e:\n",
    "        if os.path.exists(corpus_file.name):\n",
    "            os.unlink(corpus_file.name)\n",
    "        if os.path.exists(model_file.name):\n",
    "            os.unlink(model_file.name)\n",
    "        raise e\n",
    "    finally:\n",
    "        if os.path.exists(corpus_file.name):\n",
    "            os.unlink(corpus_file.name)\n",
    "\n",
    "def setup_beam_search_decoder(vocab, train_data, wiki_data=None, order=3):\n",
    "    \"\"\"Setup beam search decoder with language model.\"\"\"\n",
    "    lmplz_path = setup_kenlm()\n",
    "    if lmplz_path is None:\n",
    "        raise RuntimeError(\"Failed to setup KenLM\")\n",
    "    \n",
    "    print(\"Preparing text corpus...\")\n",
    "    \n",
    "    # Process IAM texts\n",
    "    iam_texts = []\n",
    "    for item in train_data:\n",
    "        if item.get(\"text\"):\n",
    "            cleaned = clean_text_for_kenlm(item[\"text\"])\n",
    "            if cleaned:  \n",
    "                iam_texts.append(cleaned)\n",
    "    \n",
    "    print(f\"Cleaned IAM texts: {len(iam_texts)} samples\")\n",
    "    \n",
    "    # Process WikiText if available\n",
    "    if wiki_data:\n",
    "        print(\"Processing WikiText data...\")\n",
    "        wiki_texts = []\n",
    "        for _, x in enumerate(wiki_data):\n",
    "            if x.get('text') and x['text'].strip():\n",
    "                cleaned = clean_text_for_kenlm(x['text'])\n",
    "                if cleaned: \n",
    "                    wiki_texts.append(cleaned)\n",
    "        \n",
    "        print(f\"Cleaned WikiText: {len(wiki_texts)} samples\")\n",
    "        combined_texts = iam_texts + wiki_texts[:100000]\n",
    "    else:\n",
    "        combined_texts = iam_texts\n",
    "    \n",
    "    # Create corpus\n",
    "    corpus_text = \"\\n\".join(combined_texts)\n",
    "    print(f\"Final corpus: {len(combined_texts)} lines, {len(corpus_text)} characters\")\n",
    "    \n",
    "    # Create KenLM model\n",
    "    print(f\"Training {order}-gram language model...\")\n",
    "    model_path = create_kenlm_from_corpus(corpus_text, order=order, lmplz_path=lmplz_path)\n",
    "    \n",
    "    # Build decoder\n",
    "    print(\"Building CTC decoder...\")\n",
    "    chars = vocab[1:]  # Remove <BLANK> token\n",
    "    decoder = build_ctcdecoder(\n",
    "        labels=chars,\n",
    "        kenlm_model_path=model_path,\n",
    "        alpha=0.35,\n",
    "        beta=0.6,\n",
    "        unk_score_offset=-10.0,\n",
    "    )\n",
    "    \n",
    "    print(\"Beam search decoder ready!\")\n",
    "    return decoder, model_path\n",
    "\n",
    "def wbs_decode_batch(log_probs_btV, decoder, blank_is_first=True, temperature=1.1, chars=None):\n",
    "    \"\"\"Decode batch using beam search with language model.\"\"\"\n",
    "    # Apply temperature scaling\n",
    "    if temperature != 1.0:\n",
    "        log_probs_btV = log_probs_btV / temperature\n",
    "        log_probs_btV = torch.log_softmax(log_probs_btV, dim=-1)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    logits = log_probs_btV.detach().cpu().numpy()\n",
    "    \n",
    "    # Move blank to last position if needed\n",
    "    if blank_is_first:\n",
    "        logits = np.concatenate([logits[:, :, 1:], logits[:, :, :1]], axis=2)\n",
    "    \n",
    "    texts = []\n",
    "    for i in range(logits.shape[0]):\n",
    "        try:\n",
    "            text = decoder.decode(logits[i])\n",
    "            texts.append(text)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Beam search decode failed for batch item {i}: {e}\")\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28563eed",
   "metadata": {},
   "source": [
    "# Evaluation metrics functions\n",
    "\n",
    "This section defines evaluation metrics for handwriting recognition. The calculate_accuracy function computes both character-level and word-level accuracy by comparing predicted text sequences with ground truth, accounting for substitutions, insertions, and deletions. The word_edit_distance helper calculates the edit distance between sequences of words, enabling computation of word accuracy (WRA). Additionally, calculate_edit_distance provides a normalized edit distance (CER/WER style), giving a single metric that reflects the overall transcription error relative to the length of the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88025555",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "def word_edit_distance(pred_words, gt_words):\n",
    "    \"\"\"Calculate edit distance between word sequences.\"\"\"\n",
    "    m, n = len(pred_words), len(gt_words)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if pred_words[i-1] == gt_words[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "    \n",
    "    return dp[m][n]\n",
    "\n",
    "def calculate_accuracy(predictions, ground_truths):\n",
    "    \"\"\"Calculate character and word accuracy.\"\"\"\n",
    "    preds = [str(p) for p in predictions]\n",
    "    gts = [str(g) for g in ground_truths]\n",
    "    \n",
    "    if not preds or not gts or len(preds) != len(gts):\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    # Character accuracy\n",
    "    total_char_errors = sum(editdistance.eval(p, g) for p, g in zip(preds, gts))\n",
    "    total_gt_chars = sum(len(g) for g in gts)\n",
    "    \n",
    "    if total_gt_chars == 0:\n",
    "        char_accuracy = 1.0 if total_char_errors == 0 else 0.0\n",
    "    else:\n",
    "        cer = total_char_errors / total_gt_chars\n",
    "        char_accuracy = max(0.0, 1.0 - cer)\n",
    "    \n",
    "    # Word accuracy \n",
    "    total_word_errors = 0\n",
    "    total_gt_words = 0\n",
    "    for pred, gt in zip(preds, gts):\n",
    "        pred_words = pred.split()\n",
    "        gt_words = gt.split()\n",
    "        total_word_errors += word_edit_distance(pred_words, gt_words)\n",
    "        total_gt_words += len(gt_words)\n",
    "    \n",
    "    if total_gt_words == 0:\n",
    "        word_accuracy = 1.0 if total_word_errors == 0 else 0.0\n",
    "    else:\n",
    "        wer = total_word_errors / total_gt_words\n",
    "        word_accuracy = max(0.0, 1.0 - wer)\n",
    "    \n",
    "    return char_accuracy, word_accuracy\n",
    "\n",
    "def calculate_edit_distance(predictions, ground_truths):\n",
    "    \"\"\"Calculate normalized edit distance.\"\"\"\n",
    "    total_distance = 0\n",
    "    total_length = 0\n",
    "    \n",
    "    for pred, gt in zip(predictions, ground_truths):\n",
    "        distance = editdistance.eval(pred, gt)\n",
    "        total_distance += distance\n",
    "        total_length += max(len(pred), len(gt), 1)\n",
    "    \n",
    "    return total_distance / max(total_length, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3bca9",
   "metadata": {},
   "source": [
    "# Training and validation helper functions\n",
    "\n",
    "This section defines the training and validation routines for the CRNN model. The train_epoch function performs one full epoch of training, computing the CTC loss, performing backpropagation, and optionally decoding predictions using the beam search decoder for interim metric evaluation. The validate_epoch function runs the model in evaluation mode on the validation set, calculating loss and metrics without updating model weights. Both functions return comprehensive metrics including average loss, character-level accuracy, word-level accuracy, and normalized edit distance, allowing detailed monitoring of model performance over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42de8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, ctc_loss, device, epoch, decoder, chars):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_predictions, all_ground_truths = [], []\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}', leave=False)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        images = batch['images'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        target_lengths = batch['target_lengths'].to(device)\n",
    "        texts = batch['texts']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)  \n",
    "        outputs_ctc = outputs.permute(1, 0, 2)  \n",
    "        \n",
    "        input_lengths = torch.full((images.size(0),), outputs_ctc.size(0), \n",
    "                         dtype=torch.long, device=device)\n",
    "        \n",
    "        # Calculate CTC loss\n",
    "        loss = ctc_loss(outputs_ctc, targets, input_lengths, target_lengths)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Collect predictions for metrics (every 10 batches)\n",
    "        if batch_idx % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    preds = wbs_decode_batch(outputs, decoder, blank_is_first=True, chars=chars)\n",
    "                    all_predictions.extend(preds)\n",
    "                    all_ground_truths.extend(texts)\n",
    "                except RuntimeError:\n",
    "                    pass  # Skip this batch for metrics\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Avg Loss': f'{total_loss/(batch_idx+1):.4f}'\n",
    "        })\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    avg_loss = total_loss / num_batches\n",
    "    if all_predictions:\n",
    "        char_acc, word_acc = calculate_accuracy(all_predictions, all_ground_truths)\n",
    "        edit_dist = calculate_edit_distance(all_predictions, all_ground_truths)\n",
    "    else:\n",
    "        char_acc, word_acc, edit_dist = 0.0, 0.0, 0.0\n",
    "    \n",
    "    return avg_loss, char_acc, word_acc, edit_dist\n",
    "\n",
    "def validate_epoch(model, val_loader, ctc_loss, device, decoder, chars):\n",
    "    \"\"\"Validation epoch.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions, all_ground_truths = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validation', leave=False):\n",
    "            images = batch['images'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            target_lengths = batch['target_lengths'].to(device)\n",
    "            texts = batch['texts']\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)                 \n",
    "            outputs_ctc = outputs.permute(1, 0, 2) \n",
    "\n",
    "            input_lengths = torch.full((images.size(0),), outputs_ctc.size(0), \n",
    "                                     dtype=torch.long, device=device)                         \n",
    "\n",
    "            # Calculate loss\n",
    "            loss = ctc_loss(outputs_ctc, targets, input_lengths, target_lengths)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Decode predictions\n",
    "            try:\n",
    "                preds = wbs_decode_batch(outputs, decoder, blank_is_first=True, chars=chars)\n",
    "                all_predictions.extend(preds)\n",
    "                all_ground_truths.extend(texts)\n",
    "            except RuntimeError:\n",
    "                all_predictions.extend([\"\"] * len(texts))\n",
    "                all_ground_truths.extend(texts)\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    char_acc, word_acc = calculate_accuracy(all_predictions, all_ground_truths)\n",
    "    edit_dist = calculate_edit_distance(all_predictions, all_ground_truths)\n",
    "\n",
    "    return avg_loss, char_acc, word_acc, edit_dist, all_predictions[:10], all_ground_truths[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f88e6f",
   "metadata": {},
   "source": [
    "# Checkpointing and Training History Visualization\n",
    "\n",
    "This section provides utilities for saving model checkpoints and visualizing training progress. The save_checkpoint function stores the model state, optimizer state, epoch number, and loss metrics to disk, enabling resuming or inspecting training later. The plot_training_history function creates a comprehensive visualization of the training and validation metrics over epochs, including loss, character accuracy, word accuracy, and edit distance, helping monitor model performance and detect potential overfitting or underfitting trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d9e712",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, save_dir):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss\n",
    "    }\n",
    "    \n",
    "    save_path = Path(save_dir) / f'checkpoint_epoch_{epoch}.pth'\n",
    "    torch.save(checkpoint, save_path)\n",
    "    return save_path\n",
    "\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"Plot training history.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0,0].plot(history['train_loss'], label='Train Loss', color='blue')\n",
    "    axes[0,0].plot(history['val_loss'], label='Val Loss', color='red')\n",
    "    axes[0,0].set_title('Loss')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True)\n",
    "    \n",
    "    # Character Accuracy\n",
    "    axes[0,1].plot(history['train_char_acc'], label='Train Char Acc', color='blue')\n",
    "    axes[0,1].plot(history['char_acc'], label='Val Char Acc', color='red')\n",
    "    axes[0,1].set_title('Character Accuracy')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('Accuracy')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True)\n",
    "    \n",
    "    # Word Accuracy\n",
    "    axes[1,0].plot(history['train_word_acc'], label='Train Word Acc', color='blue')\n",
    "    axes[1,0].plot(history['word_acc'], label='Val Word Acc', color='red')\n",
    "    axes[1,0].set_title('Word Accuracy')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Accuracy')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True)\n",
    "    \n",
    "    # Edit Distance\n",
    "    axes[1,1].plot(history['train_edit_dist'], label='Train Edit Dist', color='blue')\n",
    "    axes[1,1].plot(history['edit_dist'], label='Val Edit Dist', color='red')\n",
    "    axes[1,1].set_title('Edit Distance')\n",
    "    axes[1,1].set_xlabel('Epoch')\n",
    "    axes[1,1].set_ylabel('Edit Distance')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475ae0c",
   "metadata": {},
   "source": [
    "# Main training function\n",
    "\n",
    "This section implements the full training pipeline for the CRNN model. The train_model function orchestrates multiple epochs of training and validation, integrates the CTC loss, optimizer, and learning rate scheduler, and monitors metrics such as character accuracy, word accuracy, and edit distance. It also handles early stopping, checkpointing the best model, displaying sample predictions, and saving the training history. At the end, the function visualizes the training progress and returns both the recorded metrics and the path to the best-performing model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45db62",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, decoder, chars,\n",
    "                num_epochs=80, learning_rate=3e-4, weight_decay=1e-4, \n",
    "                save_dir='checkpoints', patience=7, min_delta=0):\n",
    "    \"\"\"Complete training function.\"\"\"\n",
    "    \n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    Path(save_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    ctc_loss = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=5, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_char_acc': [], 'train_word_acc': [], 'train_edit_dist': [],\n",
    "        'char_acc': [], 'word_acc': [], 'edit_dist': []\n",
    "    }\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_path = None\n",
    "    \n",
    "    print(f\"Starting training on {device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        train_loss, train_char_acc, train_word_acc, train_edit_dist = train_epoch(\n",
    "            model, train_loader, optimizer, ctc_loss, device, epoch, decoder, chars\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, char_acc, word_acc, edit_dist, pred_samples, gt_samples = validate_epoch(\n",
    "            model, val_loader, ctc_loss, device, decoder, chars\n",
    "        )\n",
    "        \n",
    "        scheduler.step(epoch + 1)\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_char_acc'].append(train_char_acc)\n",
    "        history['train_word_acc'].append(train_word_acc)\n",
    "        history['train_edit_dist'].append(train_edit_dist)\n",
    "        history['char_acc'].append(char_acc)\n",
    "        history['word_acc'].append(word_acc)\n",
    "        history['edit_dist'].append(edit_dist)\n",
    "        \n",
    "        # Print results\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs} ({epoch_time:.1f}s)\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Train - Char: {train_char_acc:.3f} | Word: {train_word_acc:.3f} | Edit: {train_edit_dist:.3f}\")\n",
    "        print(f\"Val   - Char: {char_acc:.3f} | Word: {word_acc:.3f} | Edit: {edit_dist:.3f}\")\n",
    "        print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Show sample predictions\n",
    "        print(\"\\nSample Predictions:\")\n",
    "        for i in range(min(3, len(pred_samples))):\n",
    "            print(f\"GT:   '{gt_samples[i]}'\")\n",
    "            print(f\"Pred: '{pred_samples[i]}'\")\n",
    "            print()\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = save_checkpoint(model, optimizer, epoch, train_loss, val_loss, save_dir)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_path = checkpoint_path\n",
    "            print(f\" New best model saved (val_loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement ({epochs_without_improvement}/{patience})\")\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = Path(save_dir) / 'training_history.json'\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    \n",
    "    plot_training_history(history, save_path=Path(save_dir) / 'training_plot.png')\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Best model: {best_model_path}\")\n",
    "    print(f\"Training history saved: {history_path}\")\n",
    "    \n",
    "    return history, best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7dce6",
   "metadata": {},
   "source": [
    "# Test Sample Evaluation\n",
    "\n",
    "This section defines a function to evaluate the trained model on test samples. The evaluate_test_samples function randomly selects a subset of examples from the dataset, runs inference using the beam search decoder, and computes the edit distance for each sample. It also calculates corpus-level metrics including character accuracy, word accuracy, and normalized edit distance, providing a quantitative assessment of the model's performance on unseen data. The function returns detailed results for individual samples along with the overall metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64025ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_test_samples(model, dataset, device, decoder, chars, num_samples=10):\n",
    "    \"\"\"Evaluate model on test samples.\"\"\"\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    results = []\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_ground_truths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in indices:\n",
    "            sample = dataset[idx]\n",
    "            image = sample['image'].unsqueeze(0).to(device)\n",
    "            gt = sample['text']\n",
    "            output = model(image)  \n",
    "            pred = wbs_decode_batch(output, decoder, blank_is_first=True, chars=chars)[0]\n",
    "            \n",
    "            ed = editdistance.eval(pred, gt)\n",
    "            \n",
    "            results.append({\n",
    "                'prediction': pred, \n",
    "                'ground_truth': gt, \n",
    "                'edit_distance': ed\n",
    "            })\n",
    "            \n",
    "            all_predictions.append(pred)\n",
    "            all_ground_truths.append(gt)\n",
    "    \n",
    "    # Calculate corpus-level metrics\n",
    "    char_accuracy, word_accuracy = calculate_accuracy(all_predictions, all_ground_truths)\n",
    "    edit_distance = calculate_edit_distance(all_predictions, all_ground_truths)\n",
    "    \n",
    "    print(f\"Corpus Char Accuracy: {char_accuracy:.3f}\")\n",
    "    print(f\"Corpus Word Accuracy: {word_accuracy:.3f}\")\n",
    "    print(f\"Corpus Edit Distance: {edit_distance:.3f}\")\n",
    "    print(f\"Avg Edit Distance per sample: {np.mean([r['edit_distance'] for r in results]):.3f}\")\n",
    "    \n",
    "    # Add corpus metrics to results\n",
    "    for result in results:\n",
    "        result['corpus_char_accuracy'] = char_accuracy\n",
    "        result['corpus_word_accuracy'] = word_accuracy\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f5762",
   "metadata": {},
   "source": [
    "# Main Training and Evaluation Pipeline\n",
    "\n",
    "This section orchestrates the entire training and evaluation workflow. The main function handles environment setup, dataset loading, exploratory data analysis, and creation of training, validation, and test datasets. It sets up the beam search decoder with an optional language model, initializes the CRNN model, and runs the complete training loop with checkpointing and early stopping. Finally, it evaluates the trained model on test samples, reporting metrics such as character accuracy, word accuracy, and edit distance, providing a comprehensive end-to-end demonstration of the handwriting recognition pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261123f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline.\"\"\"\n",
    "    \n",
    "    # Setup environment\n",
    "    setup_environment()\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Loading IAM dataset...\")\n",
    "    dataset_all = load_dataset(\"Teklia/IAM-line\")\n",
    "    \n",
    "    train_hf = dataset_all[\"train\"]\n",
    "    val_hf = dataset_all[\"validation\"]\n",
    "    test_hf = dataset_all[\"test\"]\n",
    "    full_dataset = concatenate_datasets([train_hf, val_hf, test_hf])\n",
    "    \n",
    "    print(f\"Train split: {len(train_hf)} samples\")\n",
    "    print(f\"Val split:   {len(val_hf)} samples\") \n",
    "    print(f\"Test split:  {len(test_hf)} samples\")\n",
    "    print(f\"Total samples: {len(full_dataset)}\")\n",
    "    \n",
    "    # Show vocabulary info\n",
    "    print(f\"Vocab size (incl. blank): {VOCAB_SIZE}\")\n",
    "    print(f\"Characters: {''.join(CHARS)}\")\n",
    "    \n",
    "    # Exploratory data analysis\n",
    "    print(\"\\n=== EXPLORATORY DATA ANALYSIS ===\")\n",
    "    show_random_samples(full_dataset, n=5)\n",
    "    analyze_dataset(full_dataset)\n",
    "    \n",
    "    # Setup beam search decoder\n",
    "    print(\"\\n=== SETTING UP BEAM SEARCH DECODER ===\")\n",
    "    try:\n",
    "        print(\"Loading WikiText...\")\n",
    "        wikitext = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")\n",
    "        print(f\"WikiText loaded: {len(wikitext)} samples\")\n",
    "    except Exception as e:\n",
    "        wikitext = None\n",
    "        print(f\"WikiText not available: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Setting up beam search decoder...\")\n",
    "        decoder, model_path = setup_beam_search_decoder(\n",
    "            vocab=VOCAB, train_data=train_hf, wiki_data=wikitext, order=3\n",
    "        )\n",
    "        print(\"Beam search decoder setup complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Setup failed: {e}\")\n",
    "        print(\"Using basic CTC decoder...\")\n",
    "        from pyctcdecode import build_ctcdecoder\n",
    "        decoder = build_ctcdecoder(labels=CHARS)\n",
    "        model_path = None\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\n=== CREATING DATASETS ===\")\n",
    "    train_dataset = IAMDataset(\n",
    "        train_hf, CHAR_TO_IDX, IDX_TO_CHAR,\n",
    "        augment=True, enhance_contrast=True, binarization=\"adaptive_gaussian\"\n",
    "    )\n",
    "    \n",
    "    val_dataset = IAMDataset(\n",
    "        val_hf, CHAR_TO_IDX, IDX_TO_CHAR,\n",
    "        augment=False, enhance_contrast=True, binarization=\"adaptive_gaussian\"\n",
    "    )\n",
    "    \n",
    "    test_dataset = IAMDataset(\n",
    "        test_hf, CHAR_TO_IDX, IDX_TO_CHAR,\n",
    "        augment=False, enhance_contrast=True, binarization=\"adaptive_gaussian\"\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        train_dataset, val_dataset, batch_size=BATCH_SIZE, num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Show preprocessing examples\n",
    "    show_preprocessed_samples(full_dataset, n=3)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Test data loading\n",
    "    print(\"\\nTesting data loading...\")\n",
    "    batch = next(iter(train_loader))\n",
    "    print(f\"Batch images shape: {batch['images'].shape}\")\n",
    "    print(f\"Sample text: '{batch['texts'][0]}'\")\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"\\n=== INITIALIZING MODEL ===\")\n",
    "    model = CRNN(\n",
    "        vocab_size=VOCAB_SIZE, \n",
    "        hidden_size=512, \n",
    "        num_lstm_layers=2,\n",
    "        dropout=DROPOUT,\n",
    "        use_attention=True\n",
    "    )\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\n=== STARTING TRAINING ===\")\n",
    "    history, best_model_path = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        decoder=decoder,\n",
    "        chars=CHARS,\n",
    "        num_epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        save_dir='checkpoints',\n",
    "        patience=6\n",
    "    )\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\n=== FINAL EVALUATION ===\")\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"Evaluating on test samples:\")\n",
    "    evaluate_test_samples(model, test_dataset, device, decoder, CHARS, num_samples=100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
